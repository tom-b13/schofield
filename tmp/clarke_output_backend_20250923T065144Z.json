{
  "tests": [
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_migration_column_types_match_erd_types",
      "predicate": "assert table in tables, f\"CREATE TABLE missing for entity {ent['name']}\"",
      "evidence": {
        "matches": [
          {
            "structure": "assertion",
            "name": "entity_table_presence_check",
            "lines": "231–241",
            "excerpt": "for f in ent[\"fields\"]:\n    fname = f.get(\"name\")\n    ftype = f.get(\"type\")\n    assert fname and ftype, \"Each field must have name and type\"\n    table = ent[\"name\"].lower()\n    assert table in tables, f\"CREATE TABLE missing for entity {ent['name']}\""
          },
          {
            "structure": "convention",
            "name": "db_case_snake_case",
            "lines": "3–8",
            "excerpt": "\"conventions\": {\n  \"db_case\": \"snake_case\",\n  \"entities_case\": \"CamelCase\",\n  \"placeholder_format\": \"UPPERCASE_UNDERSCORE\",\n  \"enums\": {\"answer_kind\": [\"boolean\", \"enum_single\", \"long_text\", \"number\", \"short_string\"]}\n}"
          },
          {
            "structure": "SQL DDL",
            "name": "001_init.sql_tables",
            "lines": "14–40 (excerpt)",
            "excerpt": "CREATE TABLE IF NOT EXISTS answer_option (\n    option_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    question_id UUID NOT NULL,\n    value TEXT NOT NULL,\n    label TEXT NULL,\n    sort_index INT NOT NULL DEFAULT 0\n);\n...\nCREATE TABLE IF NOT EXISTS questionnaire_question (\n    question_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    ...\n);"
          }
        ],
        "passable_by_app_code": false
      },
      "next_step": "update test code",
      "instructions_to_ada": "In test_migration_column_types_match_erd_types, convert entity names to snake_case before comparing to DDL table names. Replace `table = ent[\"name\"].lower()` with a snake_case transform, e.g. `table = re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", ent[\"name\"]).lower()`; then assert against `tables` using that value.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_foreign_keys_modelled_and_present_in_ddl",
      "predicate": "assert any(cols == c.replace(\" \", \"\").lower() and ref[\"entity\"].lower() == t.lower() and refcols == rc.replace(\" \", \"\").lower() for c, t, rc in _find_fk_defs(sql)), f\"FK {fk['name']} missing in 002_constraints.sql\"",
      "evidence": {
        "matches": [
          {
            "structure": "assertion",
            "name": "fk_match_against_sql",
            "lines": "283–301",
            "excerpt": "cols = \",\".join(fk[\"columns\"]).lower()\nrefcols = \",\".join(ref[\"columns\"]).lower()\nassert any(\n    cols == c.replace(\" \", \"\").lower() and ref[\"entity\"].lower() == t.lower() and refcols == rc.replace(\" \", \"\").lower()\n    for c, t, rc in _find_fk_defs(sql)\n), f\"FK {fk['name']} missing in 002_constraints.sql\""
          },
          {
            "structure": "ERD",
            "name": "AnswerOption.fk_answer_option_question",
            "lines": "24–31",
            "excerpt": "\"foreign_keys\": [\n  {\"name\": \"fk_answer_option_question\", \"columns\": [\"question_id\"], \"references\": {\"entity\": \"QuestionnaireQuestion\", \"columns\": [\"question_id\"]}}\n]"
          },
          {
            "structure": "SQL DDL",
            "name": "002_constraints.sql_fk",
            "lines": "4–9",
            "excerpt": "ALTER TABLE answer_option\n    ADD CONSTRAINT fk_answer_option_question\n        FOREIGN KEY (question_id) REFERENCES questionnaire_question(question_id) ON DELETE CASCADE,"
          }
        ],
        "passable_by_app_code": false
      },
      "next_step": "update test code",
      "instructions_to_ada": "In test_foreign_keys_modelled_and_present_in_ddl, compare the referenced table using snake_case of ERD entity names. Replace `ref[\"entity\"].lower() == t.lower()` with `re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", ref[\"entity\"]).lower() == t.lower()`.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_indexes_declared_and_created_by_migrations",
      "predicate": "assert any(cols == c and table == t for _, t, c in create_index_cols), (f\"Index {ix['name']} missing in 003_indexes.sql\")",
      "evidence": {
        "matches": [
          {
            "structure": "assertion",
            "name": "index_match_against_sql",
            "lines": "326–343",
            "excerpt": "cols = \",\".join(ix[\"columns\"]).replace(\" \", \"\").lower()\ntable = ent.get(\"name\", \"\").lower()\nassert any(cols == c and table == t for _, t, c in create_index_cols), (\n    f\"Index {ix['name']} missing in 003_indexes.sql\"\n)"
          },
          {
            "structure": "ERD",
            "name": "AnswerOption.index",
            "lines": "31–33",
            "excerpt": "\"indexes\": [\n  {\"name\": \"ix_answer_option_question\", \"columns\": [\"question_id\"]}\n]"
          },
          {
            "structure": "SQL DDL",
            "name": "003_indexes.sql_entry",
            "lines": "6–12",
            "excerpt": "CREATE INDEX IF NOT EXISTS ix_answer_option_question ON answer_option(question_id);"
          }
        ],
        "passable_by_app_code": false
      },
      "next_step": "update test code",
      "instructions_to_ada": "In test_indexes_declared_and_created_by_migrations, derive the table name from the ERD entity via snake_case. Replace `table = ent.get(\"name\", \"\").lower()` with `table = re.sub(r\"(?<!^)(?=[A-Z])\", \"_\", ent[\"name\"]).lower()`.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_collections_are_in_deterministic_sorted_order",
      "predicate": "assert names == sorted(names), f\"Fields must be sorted by name for entity {ent.get('name')}\"",
      "evidence": {
        "matches": [
          {
            "structure": "assertion",
            "name": "fields_sorted_check",
            "lines": "433–449",
            "excerpt": "for ent in erd.get(\"entities\", []) or []:\n    fields = ent.get(\"fields\", []) or []\n    names = [f.get(\"name\", \"\") for f in fields if isinstance(f, dict)]\n    assert names == sorted(names), f\"Fields must be sorted by name for entity {ent.get('name')}\""
          },
          {
            "structure": "ERD",
            "name": "AnswerOption.fields",
            "lines": "16–23",
            "excerpt": "\"fields\": [\n  {\"name\": \"option_id\", \"type\": \"uuid\"},\n  {\"name\": \"question_id\", \"type\": \"uuid\"},\n  {\"name\": \"value\", \"type\": \"text\"},\n  {\"name\": \"label\", \"type\": \"text\"},\n  {\"name\": \"sort_index\", \"type\": \"int\"}\n]"
          }
        ],
        "passable_by_app_code": true
      },
      "next_step": "update application code",
      "instructions_to_ada": "In docs/erd_spec.json, reorder AnswerOption.fields alphabetically by field name: [label, option_id, question_id, sort_index, value]. Ensure no other entity violates this rule.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_rollback_scripts_present_and_reverse_prior_migrations",
      "predicate": "assert m is not None, f\"Rollback must drop {kind} named {name}\"",
      "evidence": {
        "matches": [
          {
            "structure": "pattern",
            "name": "drop_constraint_regex",
            "lines": "590–601",
            "excerpt": "elif kind == \"constraint\":\n    pat = rf\"DROP\\s+CONSTRAINT\\s+{re.escape(name)}\\b\"\n...\nassert m is not None, f\"Rollback must drop {kind} named {name}\""
          },
          {
            "structure": "SQL DDL",
            "name": "004_rollbacks.sql_constraint_drop",
            "lines": "22–42",
            "excerpt": "ALTER TABLE questionnaire_question\n    DROP CONSTRAINT IF EXISTS uq_question_external_qid;\n...\nALTER TABLE answer_option\n    DROP CONSTRAINT IF EXISTS uq_answer_option_question_value,\n    DROP CONSTRAINT IF EXISTS fk_answer_option_question;"
          }
        ],
        "passable_by_app_code": false
      },
      "next_step": "update test code",
      "instructions_to_ada": "In _drop_pos within test_rollback_scripts_present_and_reverse_prior_migrations, allow optional IF EXISTS for constraints: use pattern `rf\"DROP\\s+CONSTRAINT\\s+(?:IF\\s+EXISTS\\s+)?{re.escape(name)}\\b\"`.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_generated_document_storage_is_modelled",
      "predicate": "assert re.search(r\"CREATE\\s+TABLE\\s+generated_document\\b\", sql, re.IGNORECASE), \"DDL must create generated_document table\"",
      "evidence": {
        "matches": [
          {
            "structure": "assertion",
            "name": "generated_document_table_presence",
            "lines": "621–623",
            "excerpt": "sql = _read_text(MIGRATIONS / \"001_init.sql\")\nassert re.search(r\"CREATE\\s+TABLE\\s+generated_document\\b\", sql, re.IGNORECASE), \"DDL must create generated_document table\""
          },
          {
            "structure": "SQL DDL",
            "name": "001_init.sql_generated_document",
            "lines": "80–87",
            "excerpt": "CREATE TABLE IF NOT EXISTS generated_document (\n    generated_document_id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n    response_set_id UUID NOT NULL,\n    output_uri TEXT NOT NULL,\n    created_at TIMESTAMPTZ NOT NULL DEFAULT now()\n);"
          }
        ],
        "passable_by_app_code": false
      },
      "next_step": "update test code",
      "instructions_to_ada": "Relax the regex to allow optional IF NOT EXISTS between TABLE and the table name: `r\"CREATE\\s+TABLE\\s+(?:IF\\s+NOT\\s+EXISTS\\s+)?generated_document\\b\"`.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_constraints_and_indexes_live_in_migrations_not_code",
      "predicate": "assert not offenders, f\"DDL tokens must not appear outside migrations: {offenders}\"",
      "evidence": {
        "matches": [
          {
            "structure": "scanner",
            "name": "_ddl_tokens_present_outside_migrations",
            "lines": "146–167",
            "excerpt": "for path in Path(\".\").rglob(\"*.py\"):\n    if str(path).startswith(\"./migrations/\"):\n        continue\n    ...\n    for t in tokens:\n        if re.search(t, text, re.IGNORECASE):\n            offenders.append((path, t))"
          },
          {
            "structure": "failure_output",
            "name": "offenders_list",
            "lines": "raw_output@227–230",
            "excerpt": "DDL tokens must not appear outside migrations: [(PosixPath('tests/architectural/test_epic_a_data_model_architecture.py'), '\\\\bCREATE\\\\s+TABLE\\\\b'), (PosixPath('tests/integration/features/steps/epic_a_behaviour_steps.py'), '\\\\bCONSTRAINT\\\\b'), (PosixPath('tests/functional/test_epic_a_data_model_functional.py'), '\\\\bCONSTRAINT\\\\b')]"
          }
        ],
        "passable_by_app_code": false
      },
      "next_step": "update test code",
      "instructions_to_ada": "Restrict the scan to application sources by excluding the tests directory. In _ddl_tokens_present_outside_migrations, add a guard: `if str(path).startswith(\"./tests/\"):\n    continue` or constrain the glob to `Path(\"app\").rglob(\"*.py\")`.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_placeholder_uniqueness_enforced_via_partial_unique_on_questions",
      "predicate": "assert any(u.get(\"columns\") == [\"placeholder_code\"] and (u.get(\"where\") or \"\").lower().strip() in {\"placeholder_code is not null\", \"placeholder_code!=null\"} for u in uniqs if isinstance(u, dict)), \"ERD must define partial unique on QuestionnaireQuestion(placeholder_code) where placeholder_code IS NOT NULL\"",
      "evidence": {
        "matches": [
          {
            "structure": "assertion",
            "name": "erd_requires_partial_unique_where",
            "lines": "673–686",
            "excerpt": "uniqs = qq.get(\"unique\") or []\nassert any(\n    u.get(\"columns\") == [\"placeholder_code\"] and (u.get(\"where\") or \"\").lower().strip() in {\"placeholder_code is not null\", \"placeholder_code!=null\"}\n    for u in uniqs if isinstance(u, dict)\n), \"ERD must define partial unique on QuestionnaireQuestion(placeholder_code) where placeholder_code IS NOT NULL\""
          },
          {
            "structure": "ERD",
            "name": "QuestionnaireQuestion.unique_current",
            "lines": "132–138",
            "excerpt": "\"unique\": [\n  {\"name\": \"uq_question_external_qid\", \"columns\": [\"external_qid\"]}\n]"
          },
          {
            "structure": "SQL DDL",
            "name": "partial_unique_exists_in_sql",
            "lines": "33–40",
            "excerpt": "CREATE UNIQUE INDEX IF NOT EXISTS uq_question_placeholder_code\n    ON questionnaire_question(placeholder_code)\n    WHERE placeholder_code IS NOT NULL;"
          }
        ],
        "passable_by_app_code": true
      },
      "next_step": "update application code",
      "instructions_to_ada": "In docs/erd_spec.json under entity QuestionnaireQuestion, add a unique entry for the partial unique: {\"name\": \"uq_question_placeholder_code\", \"columns\": [\"placeholder_code\"], \"where\": \"placeholder_code IS NOT NULL\"}. Keep the list sorted by name.",
      "request_for_clarification": ""
    },
    {
      "test_id": "tests/architectural/test_epic_a_data_model_architecture.py::test_erd_parity_exports_correspond_to_erd_spec",
      "predicate": "assert erd_entities.issubset(csv_entities), \"All ERD entities must appear in relationships CSV\"",
      "evidence": {
        "matches": [
          {
            "structure": "assertion",
            "name": "csv_entities_superset_check",
            "lines": "720–748",
            "excerpt": "erd_entities = {e.get(\"name\") for e in erd.get(\"entities\", []) if isinstance(e, dict)}\n...\nassert erd_entities.issubset(csv_entities), \"All ERD entities must appear in relationships CSV\""
          },
          {
            "structure": "CSV",
            "name": "erd_relationships.csv_current",
            "lines": "1–13",
            "excerpt": "source,target\nAnswerOption,QuestionnaireQuestion\nGroupValue,AnswerOption\nGroupValue,FieldGroup\nGroupValue,QuestionnaireQuestion\nGroupValue,ResponseSet\nQuestionToFieldGroup,FieldGroup\nQuestionToFieldGroup,QuestionnaireQuestion\nResponse,AnswerOption\nResponse,QuestionnaireQuestion\nResponse,ResponseSet\nResponseSet,Company"
          },
          {
            "structure": "ERD",
            "name": "GeneratedDocument.fields_imply_fk",
            "lines": "58–69",
            "excerpt": "\"name\": \"GeneratedDocument\",\n\"fields\": [\n  {\"name\": \"response_set_id\", \"type\": \"uuid\"},\n  {\"name\": \"output_uri\", \"type\": \"text\", ...}\n],\n\"indexes\": [\n  {\"name\": \"ix_generated_document_set\", \"columns\": [\"response_set_id\"]}\n]"
          }
        ],
        "passable_by_app_code": true
      },
      "next_step": "update application code",
      "instructions_to_ada": "Model and encode the GeneratedDocument → ResponseSet relationship everywhere: 1) In docs/erd_spec.json add a foreign_keys entry for GeneratedDocument: {\"name\": \"fk_generated_document_set\", \"columns\": [\"response_set_id\"], \"references\": {\"entity\": \"ResponseSet\", \"columns\": [\"response_set_id\"]}}; keep lists sorted by name. 2) In migrations/002_constraints.sql add the matching FK: `ALTER TABLE generated_document ADD CONSTRAINT fk_generated_document_set FOREIGN KEY (response_set_id) REFERENCES response_set(response_set_id) ON DELETE CASCADE;` 3) Add the relation row to docs/erd_relationships.csv: `ResponseSet,GeneratedDocument` and keep rows sorted. 4) Add the Mermaid edge `ResponseSet --> GeneratedDocument` and keep edges sorted.",
      "request_for_clarification": ""
    }
  ],
  "iteration_checkpoint": {
    "continue": false
  }
}